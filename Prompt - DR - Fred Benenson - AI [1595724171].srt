1
00:00:00,008 --> 00:00:03,735
[SPEAKER_01]: Do you want me over here?

2
00:00:03,735 --> 00:00:07,862
[SPEAKER_01]: Yeah that's perfect.

3
00:00:10,500 --> 00:00:11,500
[SPEAKER_01]: Nice to meet you, Fred.

4
00:00:11,500 --> 00:00:12,320
[SPEAKER_01]: Very nice to meet you.

5
00:00:12,320 --> 00:00:13,101
[SPEAKER_01]: Thank you for being here.

6
00:00:13,101 --> 00:00:14,041
[SPEAKER_01]: Thanks for having me.

7
00:00:14,041 --> 00:00:19,122
[SPEAKER_01]: Henrik, can you just start, give us a very short round, how do you and Fred know each other?

8
00:00:19,122 --> 00:00:24,104
[SPEAKER_00]: So Fred and I have been friends and worked together briefly back in 2006-7.

9
00:00:24,104 --> 00:00:26,944
[SPEAKER_00]: I was working on a documentary about copyright.

10
00:00:26,944 --> 00:00:35,507
[SPEAKER_00]: Fred was working at a non-profit called Creative Commons, the licensing system that underpins things like Wikipedia.

11
00:00:35,507 --> 00:00:36,607
[SPEAKER_00]: And so we were in this

12
00:00:37,307 --> 00:00:43,351
[SPEAKER_00]: group of people that were very idealistic about the internet, and have stayed friends ever since.

13
00:00:43,351 --> 00:00:44,312
[SPEAKER_01]: That's perfect.

14
00:00:44,312 --> 00:00:47,013
[SPEAKER_01]: And you've been hanging around in Copenhagen for a couple of days?

15
00:00:47,013 --> 00:00:49,255
[SPEAKER_02]: Yeah, I got here a week ago, I guess.

16
00:00:49,255 --> 00:00:53,778
[SPEAKER_02]: And then I went to Sweden for a bit, but Henrik's given me an incredible tour.

17
00:00:53,778 --> 00:00:56,640
[SPEAKER_02]: We went mushroom hunting on Friday, and that was amazing.

18
00:00:56,640 --> 00:00:59,222
[SPEAKER_02]: And then I'm going to... In which forest was that?

19
00:00:59,222 --> 00:00:59,822
[SPEAKER_01]: Sysvile.

20
00:00:59,822 --> 00:01:00,522
[SPEAKER_02]: Sysvile, yeah.

21
00:01:00,522 --> 00:01:01,363
[SPEAKER_02]: Yeah, it was wonderful.

22
00:01:01,363 --> 00:01:04,485
[SPEAKER_02]: We got lots of chanterelles, but what was the Danish name for them?

23
00:01:04,933 --> 00:01:05,354
[SPEAKER_02]: Cantareller.

24
00:01:05,354 --> 00:01:05,814
[SPEAKER_02]: Cantareller, yeah.

25
00:01:05,814 --> 00:01:06,555
[SPEAKER_02]: They were amazing.

26
00:01:06,555 --> 00:01:07,496
[SPEAKER_02]: We cooked them that night.

27
00:01:07,496 --> 00:01:08,196
[SPEAKER_02]: Are they good?

28
00:01:08,196 --> 00:01:09,137
[SPEAKER_02]: Yeah, they were great.

29
00:01:09,137 --> 00:01:10,278
[SPEAKER_01]: Ah, okay, perfect.

30
00:01:10,278 --> 00:01:12,660
[SPEAKER_01]: And do you sleep on Henrik's sofa?

31
00:01:12,660 --> 00:01:14,402
[SPEAKER_02]: No, he's got an incredible guest room.

32
00:01:14,402 --> 00:01:17,164
[SPEAKER_02]: It was a lovely, it's been a lovely stay.

33
00:01:17,164 --> 00:01:19,867
[SPEAKER_00]: Som Fred fortæller her, så er han i København for at besøge mig.

34
00:01:19,867 --> 00:01:21,228
[SPEAKER_00]: Vi er gamle venner.

35
00:01:21,228 --> 00:01:23,150
[SPEAKER_00]: Vi har været ude og samlet svampe.

36
00:01:23,150 --> 00:01:25,472
[SPEAKER_00]: Og han bor i mit gæsteverelse i vores hus.

37
00:01:26,120 --> 00:01:36,504
[SPEAKER_01]: Enough with the hygge and Denmark, you've worked with artificial intelligence for so many years, first at Kickstarter, then at Y Combinator, we'll get back to that in a minute.

38
00:01:36,504 --> 00:01:41,566
[SPEAKER_01]: What were you trying to accomplish with AI within your career?

39
00:01:41,566 --> 00:01:51,610
[SPEAKER_02]: So I took a really interesting class in grad school by somebody named Dan Schiffman, courseware, he's a great teacher, and I think the class he taught was called Nature of Code.

40
00:01:52,191 --> 00:02:02,228
[SPEAKER_02]: and then programming A to Z, and he showed us how to build a spam filter, which sounds really boring, but it's a really interesting way to analyze large amounts of text.

41
00:02:02,446 --> 00:02:08,331
[SPEAKER_01]: So the spam filters we have in our Gmail, which means I don't have to look up hundreds of mails that I don't want to see.

42
00:02:08,331 --> 00:02:08,731
[SPEAKER_01]: Exactly.

43
00:02:08,731 --> 00:02:20,059
[SPEAKER_02]: It's a powerful mathematical algorithm that goes back to probability theory that can analyze an email really quickly and say, it's more likely than not that this email is spam.

44
00:02:20,059 --> 00:02:22,741
[SPEAKER_02]: So I learned how to do some of that math.

45
00:02:22,741 --> 00:02:26,444
[SPEAKER_02]: And a couple of years later, I found myself at Kickstarter.

46
00:02:26,444 --> 00:02:30,047
[SPEAKER_02]: I was the second employee there, and just trying to make myself useful at a startup.

47
00:02:31,568 --> 00:02:39,970
[SPEAKER_02]: take on interesting projects, and do everything from look at our analytics, to answer queries from the database, to build a little bit of code.

48
00:02:39,970 --> 00:02:43,991
[SPEAKER_02]: And I picked up a, in America, they called it nights and weekends project.

49
00:02:43,991 --> 00:02:45,591
[SPEAKER_02]: I don't know if that's a phrase here.

50
00:02:45,591 --> 00:02:46,352
[SPEAKER_02]: It's probably.

51
00:02:46,352 --> 00:02:47,572
[SPEAKER_02]: No, we take the weekends off here.

52
00:02:47,572 --> 00:02:47,752
[SPEAKER_00]: I know.

53
00:02:47,752 --> 00:02:48,432
[SPEAKER_00]: It's like probably.

54
00:02:48,432 --> 00:02:49,492
[SPEAKER_00]: Yeah, we don't know that, yeah.

55
00:02:49,492 --> 00:02:50,733
[SPEAKER_01]: We have no age comps.

56
00:02:50,733 --> 00:02:50,853
[SPEAKER_01]: Yes.

57
00:02:52,413 --> 00:02:55,835
[SPEAKER_00]: Fred studied at the university and wrote code.

58
00:02:55,835 --> 00:03:07,103
[SPEAKER_00]: He was interested in, among other things, a course he took at the university, how spam filters work, and how to use algorithms and mathematical analysis of text.

59
00:03:07,103 --> 00:03:12,567
[SPEAKER_00]: He basically learned to write things that could figure out whether an email was in order or not.

60
00:03:13,712 --> 00:03:19,755
[SPEAKER_02]: I took on a nights and weekends project at Kickstarter where I was like, we're getting all these incoming projects and most of them are good.

61
00:03:19,755 --> 00:03:26,098
[SPEAKER_02]: And, you know, we're trying to, in the early days, decide which ones were a Kickstarter project and which ones weren't a Kickstarter project.

62
00:03:26,098 --> 00:03:27,578
[SPEAKER_02]: And they took this very seriously.

63
00:03:27,578 --> 00:03:29,279
[SPEAKER_02]: It was like, it has to be a creative project.

64
00:03:29,279 --> 00:03:32,060
[SPEAKER_02]: You can't raise money for your vacation.

65
00:03:32,060 --> 00:03:33,381
[SPEAKER_02]: It has to be for a documentary.

66
00:03:33,381 --> 00:03:35,542
[SPEAKER_02]: So people would come up with funny ways to do that.

67
00:03:35,922 --> 00:03:43,244
[SPEAKER_00]: Fred had a hobby project besides his work where he tried to make an artificial intelligence that could scan the many.

68
00:03:43,244 --> 00:03:53,547
[SPEAKER_00]: Suddenly Kickstarter became big and very popular and they had to find a way to automatically find out which projects were probably Kickstarter projects and which were to be chosen.

69
00:03:53,547 --> 00:03:56,928
[SPEAKER_02]: So I had a friend who worked at Y Combinator and he said, well,

70
00:03:57,668 --> 00:04:04,570
[SPEAKER_02]: We're interested in doing something similar with it with an algorithm so that it looks at incoming people who are applying to Y Combinator Would you be interested in working on that?

71
00:04:04,570 --> 00:04:18,133
[SPEAKER_02]: I was like, yeah, I mean I was always interested in living in California So I took them up on it and I moved out there and that was in 2016 So I worked a little bit on that but then I ended up kind of being part of the whole Y Combinator process and meeting there and that's how I met Sam Altman

72
00:04:18,647 --> 00:04:20,609
[SPEAKER_01]: Yeah, that was just my clue.

73
00:04:20,609 --> 00:04:21,930
[SPEAKER_01]: Let's talk about Sam Altman.

74
00:04:21,930 --> 00:04:22,230
[SPEAKER_01]: Sure.

75
00:04:22,230 --> 00:04:26,353
[SPEAKER_01]: Your first meet-up with Sam Altman, what was that?

76
00:04:26,353 --> 00:04:30,957
[SPEAKER_02]: You know, he interviewed me when I was applying for the job at YC.

77
00:04:30,957 --> 00:04:38,042
[SPEAKER_02]: And I remember we were in a tiny room and he was rolling around on one of those hoverboards.

78
00:04:38,042 --> 00:04:38,583
[SPEAKER_01]: Do you remember those?

79
00:04:38,583 --> 00:04:39,423
[SPEAKER_02]: A hoverboard?

80
00:04:39,423 --> 00:04:40,924
[SPEAKER_02]: It's not an actual hoverboard.

81
00:04:40,924 --> 00:04:42,105
[SPEAKER_02]: It's the one with the wheels.

82
00:04:42,105 --> 00:04:42,385
[SPEAKER_02]: Okay.

83
00:04:42,385 --> 00:04:45,248
[SPEAKER_02]: It was just very funny to be doing an interview while he was going around the room.

84
00:04:45,828 --> 00:04:46,429
[SPEAKER_02]: I was impressed.

85
00:04:46,429 --> 00:04:50,513
[SPEAKER_02]: I mean, I thought he, um, you know, we just, we stayed friendly while I worked there.

86
00:04:50,513 --> 00:05:03,907
[SPEAKER_02]: And, um, I saw him do what Henrik was talking about, which is get people to think bigger because that's, that's the risk with a startup is if you don't think big enough, then suddenly you're just talking about sleeping on couches instead of the hotel industry.

87
00:05:03,907 --> 00:05:04,127
[SPEAKER_02]: Right.

88
00:05:04,127 --> 00:05:05,729
[SPEAKER_02]: Which is a lot bigger than sleeping on couches.

89
00:05:06,269 --> 00:05:16,497
[SPEAKER_02]: And so that was kind of my first impression was like his skill was getting people to think creatively about how big a vision could be and kind of coaxing them to that point.

90
00:05:16,497 --> 00:05:30,269
[SPEAKER_00]: So Fred's first meeting with Sam Altman is in 2016 for his job interview, where Sam Altman meets up on one of these electric skateboards, hoverboards, where he drives around in the room while they're having this job interview.

91
00:05:30,269 --> 00:05:34,112
[SPEAKER_00]: What catches Fred in the same way is that he is incredibly good at thinking big

92
00:05:34,753 --> 00:05:37,085
[SPEAKER_01]: Try to describe him as a person.

93
00:05:37,085 --> 00:05:37,890
[SPEAKER_01]: What kind of person is he?

94
00:05:38,587 --> 00:05:46,713
[SPEAKER_02]: You could tell he was interested in kind of the abstract big ideas and maybe not as interested in the kind of day-to-day.

95
00:05:46,713 --> 00:05:49,856
[SPEAKER_01]: A little bit like Steve Jobs, kind of.

96
00:05:49,856 --> 00:05:50,336
[SPEAKER_01]: Possibly.

97
00:05:50,336 --> 00:05:52,878
[SPEAKER_02]: I've never met Steve Jobs.

98
00:05:52,878 --> 00:05:53,479
[SPEAKER_02]: I've heard about him.

99
00:05:53,479 --> 00:05:54,199
[SPEAKER_02]: I've read a book or two.

100
00:05:54,199 --> 00:05:58,623
[SPEAKER_02]: But yeah, I mean, I think he was thinking as big as it could be.

101
00:05:58,623 --> 00:06:05,148
[SPEAKER_02]: And I think AI was particularly interesting for that reason, because around that time, within a couple of years,

102
00:06:06,649 --> 00:06:09,550
[SPEAKER_02]: it looked within sight to do something really big with AI.

103
00:06:09,550 --> 00:06:10,891
[SPEAKER_02]: And I think that's what attracted him.

104
00:06:10,891 --> 00:06:16,713
[SPEAKER_02]: And it wasn't surprising for me to hear that he was shifting to open AI after YC.

105
00:06:16,713 --> 00:06:25,176
[SPEAKER_01]: I'd say Sam is up there with Mark Zuckerberg, Elon Musk, Sundar Pichai from Google, the biggest tech bosses in the world.

106
00:06:25,176 --> 00:06:29,398
[SPEAKER_01]: If you had to compare him to, say, Zuckerberg, how do they differ?

107
00:06:29,398 --> 00:06:32,279
[SPEAKER_02]: I mean, I've only met Zuckerberg once.

108
00:06:32,279 --> 00:06:34,239
[SPEAKER_02]: And he was perfectly nice to me.

109
00:06:34,239 --> 00:06:35,420
[SPEAKER_02]: But I don't think we got along as

110
00:06:36,975 --> 00:06:40,677
[SPEAKER_02]: as well as I, for whatever reason, as well as I got along with Sam.

111
00:06:40,677 --> 00:06:47,780
[SPEAKER_02]: And I think Sam seems acutely interested in listening to the concerns people have.

112
00:06:47,780 --> 00:06:57,024
[SPEAKER_02]: I mean, if your audience is kind of interested in reading more, he did a great interview with Ezra Klein of the New York Times, which I think is worth listening to or reading.

113
00:06:57,724 --> 00:07:01,729
[SPEAKER_02]: And you can see him and Ezra going back and forth on like some of these big picture concerns.

114
00:07:01,729 --> 00:07:06,634
[SPEAKER_02]: And, you know, he had some of these comments in Congress about, OK, well, maybe you're right.

115
00:07:06,634 --> 00:07:08,516
[SPEAKER_02]: Maybe this is concerning or this is concerning.

116
00:07:08,516 --> 00:07:10,758
[SPEAKER_02]: And I think I think those are are genuine.

117
00:07:10,758 --> 00:07:12,320
[SPEAKER_02]: I also think it's

118
00:07:12,620 --> 00:07:17,863
[SPEAKER_02]: in OpenAI's interest now to kind of ask some of those questions now that they've ended up dominant.

119
00:07:17,863 --> 00:07:22,365
[SPEAKER_02]: So he's also very smart and knows how to play chess really well, both literally and figuratively.

120
00:07:22,365 --> 00:07:32,449
[SPEAKER_00]: Fred fortæller at Sam Altman, måske virkeligen adskiller sig mest for de andre tech-chefer ved at han faktisk bekymrer sig om hvad folk vil ha, hvad folk tænker.

121
00:07:32,449 --> 00:07:39,793
[SPEAKER_00]: Han mødte for nylig opp i kongressen i en høring om truslen for kunstig intelligens, og sagde jo at han mente at der var klare problemer

122
00:07:40,654 --> 00:07:47,419
[SPEAKER_00]: OpenAI, the folks behind ChatGPT, kicked off as non-profit, aiming to make friendly AI that helps everyone.

123
00:07:47,419 --> 00:07:50,321
[SPEAKER_00]: They didn't just want Google and Facebook to have all the fun with AI.

124
00:08:04,630 --> 00:08:18,373
[SPEAKER_01]: And later they shifted gears and went for what they call for-profit or capped profit, which means that there's kind of a lead to how much cash the investors can make and anything extra goes back to the organization's original purpose.

125
00:08:18,373 --> 00:08:28,876
[SPEAKER_01]: And OpenAI says they made the switch because they needed it for server capacity, high priced research, stuff like that to train those large language models.

126
00:08:28,876 --> 00:08:34,337
[SPEAKER_01]: Doesn't that just prove that at the end of the day, no matter how noble the cause, it's all about the money?

127
00:08:35,778 --> 00:08:38,179
[SPEAKER_02]: I mean, I think that's putting too fine a point on it.

128
00:08:38,179 --> 00:08:49,285
[SPEAKER_02]: I mean, if you look at how much it actually cost to get OpenAI to where they are right now relative to that, it's actually a little bit on the smaller side.

129
00:08:49,285 --> 00:08:51,046
[SPEAKER_02]: I mean, it's billions of dollars probably.

130
00:08:51,046 --> 00:08:55,088
[SPEAKER_02]: I mean, I think they took over $10 billion from Microsoft.

131
00:08:55,088 --> 00:09:01,071
[SPEAKER_02]: But it's a little bit unclear where it's all going to shake out in terms of actual profit.

132
00:09:01,071 --> 00:09:04,153
[SPEAKER_02]: I think something that Sam talks a lot about that I'm skeptical of is,

133
00:09:04,693 --> 00:09:07,734
[SPEAKER_02]: is this will just obviously generate wealth in the future.

134
00:09:07,734 --> 00:09:09,454
[SPEAKER_02]: And I'm not sure about that.

135
00:09:09,454 --> 00:09:16,395
[SPEAKER_02]: I think it's an interesting tool and I've been enjoying using it and I think it portends really interesting things for the future in technology.

136
00:09:16,395 --> 00:09:24,557
[SPEAKER_02]: But the idea that it's just going to generate outsized profits and money is like, the jury is still out on that one for me.

137
00:09:24,557 --> 00:09:29,898
[SPEAKER_02]: So I think there's a lot of interest and there's a lot of belief that it will generate huge amounts of money, but we'll see.

138
00:09:30,729 --> 00:09:51,393
[SPEAKER_00]: OpenAI started as a non-profit, a company that couldn't make money, and when it became clear to them that it was expensive to train these models, to attract the right scientists and employees, and to make Microsoft knock on the door with a lot of money, then OpenAI changed, which among other things was financed by Elon Musk and others, to be for-profit, but with a profit-loft.

139
00:09:51,833 --> 00:10:01,919
[SPEAKER_00]: And Fred asks himself if any of the ideas Sam Altman has about how extremely profitable AI is and transformative for society, if they will actually happen.

140
00:10:03,118 --> 00:10:09,621
[SPEAKER_01]: In Silicon Valley, they talk about the risk that we could all be finished all because of AI.

141
00:10:09,621 --> 00:10:14,623
[SPEAKER_01]: They contemplate the end of humanity, and they call it P-Doom.

142
00:10:14,623 --> 00:10:15,963
[SPEAKER_01]: The probability of doom.

143
00:10:15,963 --> 00:10:17,324
[SPEAKER_01]: Exactly.

144
00:10:17,324 --> 00:10:21,866
[SPEAKER_01]: Fred, as a person who I assume dine and chat with people in Silicon Valley,

145
00:10:22,546 --> 00:10:25,129
[SPEAKER_01]: Can you describe the vibe right now?

146
00:10:25,129 --> 00:10:28,292
[SPEAKER_01]: Are people anxious, optimistic?

147
00:10:28,292 --> 00:10:29,073
[SPEAKER_02]: How is it?

148
00:10:29,073 --> 00:10:39,584
[SPEAKER_02]: Well, I think there's a risk for the folks who are saying the sky is falling because it's kind of hard to prove one way or another.

149
00:10:39,584 --> 00:10:40,425
[SPEAKER_01]: Where it's not happening.

150
00:10:40,425 --> 00:10:44,829
[SPEAKER_02]: Yeah, it's like if it's not happening now, I mean, so we have to like project and look into the future and kind of

151
00:10:45,650 --> 00:10:46,511
[SPEAKER_02]: figure that out.

152
00:10:46,511 --> 00:10:54,576
[SPEAKER_02]: And I think there's a lot of incentive for people to say, hey, you guys aren't thinking enough about this.

153
00:10:54,576 --> 00:11:00,760
[SPEAKER_02]: And like, pay attention to me, because then they get to kind of ride on the coattails of the AI conversation by just saying the opposite.

154
00:11:00,760 --> 00:11:04,923
[SPEAKER_02]: So it's a, you kind of have to like figure out like, okay, how likely is this?

155
00:11:04,923 --> 00:11:07,905
[SPEAKER_02]: And you have to look at the technology and you have to look at how people are actually using it.

156
00:11:08,505 --> 00:11:14,787
[SPEAKER_02]: And right now, for the better, everyone is using AI with a human in the loop.

157
00:11:14,787 --> 00:11:19,729
[SPEAKER_02]: And I think that's a good phrase for your audience to understand and kind of process.

158
00:11:19,729 --> 00:11:24,851
[SPEAKER_02]: And what that means is that when I sit down to use ChatGPT, I am directing ChatGPT.

159
00:11:24,851 --> 00:11:29,853
[SPEAKER_02]: I'm saying, hey, help me write this code, or reformat this email, or whatever.

160
00:11:29,853 --> 00:11:35,795
[SPEAKER_02]: We're not at the point where we're just letting AI be an agent in the world with its own motivations and its own desires.

161
00:11:36,355 --> 00:11:39,259
[SPEAKER_02]: And I think that's actually where things could go awry.

162
00:11:39,259 --> 00:11:44,925
[SPEAKER_02]: I, having worked with a lot of AI systems, more often than not, they just make, like, dumb decisions.

163
00:11:44,925 --> 00:11:48,909
[SPEAKER_02]: They're not, you know, not necessarily, like, malicious.

164
00:11:48,909 --> 00:11:56,798
[SPEAKER_02]: And it's hard, like, for me to imagine, just with my experience of it going from a stupid or bad decision to a malicious decision.

165
00:11:57,418 --> 00:12:09,648
[SPEAKER_02]: And I think, you know, this is where alignment, the conversation around alignment, which is this kind of code word in the AI world of, does the AI's motivation align with the human's motivation?

166
00:12:09,648 --> 00:12:16,773
[SPEAKER_02]: And this goes all the way back to science fiction from the 50s and 60s around, will the robots always listen to us?

167
00:12:16,773 --> 00:12:21,076
[SPEAKER_02]: And I think OpenAI, to their credit, has spent a lot of time making sure that they like,

168
00:12:21,977 --> 00:12:25,400
[SPEAKER_02]: factor this into the way that they're building AI.

169
00:12:25,400 --> 00:12:27,482
[SPEAKER_02]: And there are a lot of smart people thinking about it.

170
00:12:27,482 --> 00:12:32,367
[SPEAKER_02]: Honestly, to answer your question about the vibe, I think a lot of people are really kind of bothered by the Doomers.

171
00:12:32,367 --> 00:12:37,111
[SPEAKER_01]: And they're just kind of... What do you mean, bothered by the Doomers?

172
00:12:37,111 --> 00:12:38,692
[SPEAKER_02]: I think some of them

173
00:12:39,553 --> 00:12:48,699
[SPEAKER_02]: some of the demands from the people who think that AI doom is coming and some of the projections about what we should do in that event get a little wonky.

174
00:12:48,699 --> 00:12:53,463
[SPEAKER_02]: There was somebody I think I saw advocating for bombing data centers.

175
00:12:53,463 --> 00:12:54,043
[SPEAKER_02]: Oh my god.

176
00:12:54,043 --> 00:12:54,323
[SPEAKER_02]: Yeah.

177
00:12:54,323 --> 00:12:56,164
[SPEAKER_02]: Is that a movement?

178
00:12:56,164 --> 00:12:59,587
[SPEAKER_02]: It's not a movement, but it was like a somewhat serious suggestion.

179
00:12:59,587 --> 00:13:00,207
[SPEAKER_02]: Okay.

180
00:13:00,207 --> 00:13:01,228
[SPEAKER_02]: In order to kind of

181
00:13:01,788 --> 00:13:05,151
[SPEAKER_02]: stave off the incoming AI apocalypse.

182
00:13:05,151 --> 00:13:13,998
[SPEAKER_02]: And it was kind of somewhat seriously suggested as a way that humans could defend themselves against the future.

183
00:13:15,207 --> 00:13:17,328
[SPEAKER_02]: I think stuff like that hurts the credibility.

184
00:13:17,328 --> 00:13:18,908
[SPEAKER_02]: I think there are risks around AI.

185
00:13:18,908 --> 00:13:20,329
[SPEAKER_02]: I'm not pretending there aren't risks.

186
00:13:20,329 --> 00:13:26,271
[SPEAKER_02]: I think talking about it in a hyperbolic Doomer sense doesn't move the conversation forward.

187
00:13:33,753 --> 00:13:47,958
[SPEAKER_00]: And Fred says that the mood in his circle is that you are a little tired of these people who think the sky falls down on their ears, and there are some completely crazy proposals where people, for example, say that you should bomb a data center to stop the development.

188
00:13:49,678 --> 00:13:59,122
[SPEAKER_00]: So Fred, these doomers have this naive idea.

189
00:13:59,122 --> 00:14:01,923
[SPEAKER_00]: What are the deeper and more real threats you see?

190
00:14:01,923 --> 00:14:07,125
[SPEAKER_00]: And I think you're quite aligned with Sam Altman, actually, from our conversation, so I will use you as a sort of...

191
00:14:07,986 --> 00:14:24,784
[SPEAKER_02]: amalgamate of the two of you well obviously I can't I can't speak for Sam on this stuff but having confronted AI systems going awry and working well one of the big things that everyone talks about this is not gonna be surprising is bias and I think open AI has done a lot of work to try to

192
00:14:25,424 --> 00:14:28,125
[SPEAKER_02]: make sure that the system is basically unoffensive.

193
00:14:28,125 --> 00:14:35,569
[SPEAKER_02]: And what's interesting is the narrative around AI has kind of shifted from, oh, what happens if these algorithms are racist?

194
00:14:35,569 --> 00:14:38,410
[SPEAKER_02]: Like, people aren't talking about that very much right now.

195
00:14:38,410 --> 00:14:42,132
[SPEAKER_02]: And it's because the main algorithm that everyone's working with isn't racist.

196
00:14:42,132 --> 00:14:43,833
[SPEAKER_02]: It's actually quite hard to get it to behave.

197
00:14:44,733 --> 00:14:47,034
[SPEAKER_02]: in a anti-social way in that sense.

198
00:14:47,034 --> 00:14:55,796
[SPEAKER_02]: That doesn't mean the training data and the way that the model works under the hood didn't kind of absorb all of the racism and bad things in society.

199
00:14:55,796 --> 00:14:56,336
[SPEAKER_02]: It did.

200
00:14:56,336 --> 00:15:00,917
[SPEAKER_02]: It's just that open AI has put kind of a layer on top of it to make it safe.

201
00:15:00,917 --> 00:15:03,818
[SPEAKER_02]: And there's a lot of people experimenting with these larger language models now.

202
00:15:04,698 --> 00:15:10,462
[SPEAKER_02]: an open source one from Facebook, and the way that they get released is very conservative.

203
00:15:10,462 --> 00:15:18,368
[SPEAKER_02]: You can ask it to make a joke about a panda, and it'll be like, well, you shouldn't because pandas are endangered.

204
00:15:18,368 --> 00:15:26,814
[SPEAKER_02]: So people are paying attention to those concerns, and I think it's because of journalists and activists saying, well, we've got to make sure bias isn't a problem.

205
00:15:27,034 --> 00:15:30,537
[SPEAKER_00]: One of the first concerns is that the AI systems have built in bias.

206
00:15:30,537 --> 00:15:33,379
[SPEAKER_00]: He says that OpenAI and others have actually been really good.

207
00:15:33,379 --> 00:15:35,020
[SPEAKER_00]: It's not something we discuss so much.

208
00:15:35,020 --> 00:15:41,005
[SPEAKER_00]: We saw earlier examples of how these language models very quickly ended up being racist or asocial.

209
00:15:41,005 --> 00:15:45,868
[SPEAKER_00]: That has been managed to prevent, at least with this very cautious release cycle.

210
00:15:45,868 --> 00:15:51,773
[SPEAKER_00]: That is, you do not release any language models that fall into that hole, which is a clear risk that they have bias.

211
00:15:53,121 --> 00:15:56,203
[SPEAKER_02]: What is your P-Doom?

212
00:15:56,203 --> 00:15:59,945
[SPEAKER_02]: My likelihood of doom given AI?

213
00:15:59,945 --> 00:16:04,469
[SPEAKER_02]: Oh man, it's so hard to predict the future in general.

214
00:16:04,469 --> 00:16:05,389
[SPEAKER_02]: Just give us a number, Fred.

215
00:16:06,776 --> 00:16:07,176
[SPEAKER_02]: I don't know.

216
00:16:07,176 --> 00:16:11,158
[SPEAKER_02]: I'd say between 5% and 10% there's actual harm.

217
00:16:11,158 --> 00:16:20,082
[SPEAKER_02]: Not like human civilization has gone wrong, but something at the top of society has gone off the rails in 20 years.

218
00:16:20,082 --> 00:16:21,262
[SPEAKER_02]: I'd say 5% to 10%.

219
00:16:21,262 --> 00:16:23,163
[SPEAKER_02]: We're not going to go extinct.

220
00:16:23,163 --> 00:16:25,444
[SPEAKER_02]: We're going to go extinct based on our own hand.

221
00:16:25,444 --> 00:16:27,305
[SPEAKER_02]: It's not going to be the machines hit to it.

222
00:16:27,605 --> 00:16:39,232
[SPEAKER_00]: Then the question is how far we are exposed by artificial intelligence and the real percentage says that he actually thinks that there is a 5-10% risk that something in society goes seriously wrong within 20 years.

223
00:16:39,232 --> 00:16:46,277
[SPEAKER_00]: Maybe not that we are exposed, but we have to take care of it ourselves, but that there is something else at a high level in society that breaks down altogether because of artificial intelligence.

224
00:16:46,756 --> 00:16:48,058
[SPEAKER_01]: I don't like that.

225
00:16:48,058 --> 00:16:50,100
[SPEAKER_02]: I mean, listen, it's a trade off, right?

226
00:16:50,100 --> 00:16:58,208
[SPEAKER_02]: Like I'm reading the Oppenheimer biography and I watch the movie and that technology was designed only to cause destruction, right?

227
00:17:09,420 --> 00:17:22,695
[SPEAKER_00]: And it's a comparison that occurs a lot in this circle with that you developed a technology that was only to be used for death and destruction, and you are afraid that Artificial Intelligence is also on the way in that direction.

228
00:17:23,735 --> 00:17:30,759
[SPEAKER_02]: So I think there's this kind of impulse to build something really powerful and that's where the two projects are kind of similar.

229
00:17:30,759 --> 00:17:37,282
[SPEAKER_02]: And then you look back and you say, wow, did we unleash something terrible and is it going to destroy society?

230
00:17:37,282 --> 00:17:43,365
[SPEAKER_02]: And I mean, nuclear energy has some of the possibility of saving society, right?

231
00:17:43,365 --> 00:17:44,426
[SPEAKER_02]: I'm a big proponent of

232
00:17:45,707 --> 00:17:51,655
[SPEAKER_02]: nuclear fusion power plants and what they could mean for sustainable energy.

233
00:17:51,655 --> 00:17:57,563
[SPEAKER_02]: But obviously they cause a huge amount of destruction in Japan and that's a serious consequence.

234
00:17:57,563 --> 00:18:00,446
[SPEAKER_02]: And we live with the threat of nuclear war.

235
00:18:00,446 --> 00:18:01,428
[SPEAKER_02]: I mean, it's just a part of

236
00:18:01,888 --> 00:18:02,589
[SPEAKER_02]: our society now.

237
00:18:02,589 --> 00:18:04,792
[SPEAKER_02]: So, you know, Marcel saying, oh, I don't like that.

238
00:18:04,792 --> 00:18:11,179
[SPEAKER_02]: I'm like, well, I don't like the fact that, you know, America has thousands of nuclear warheads just ready to go all the time.

239
00:18:11,179 --> 00:18:14,763
[SPEAKER_02]: And technology isn't unambiguously good, you know?

240
00:18:14,763 --> 00:18:21,691
[SPEAKER_02]: And I think that that's something that we've had to learn as all of the world has come online and these systems have become more powerful.

241
00:18:22,166 --> 00:18:29,772
[SPEAKER_00]: The comparison with Oppenheimer and nuclear power is good because we have seen that nuclear technology plays an important role.

242
00:18:29,772 --> 00:18:33,535
[SPEAKER_00]: It is in many ways a good energy source.

243
00:18:33,535 --> 00:18:38,418
[SPEAKER_00]: Phat says that he is a big supporter of fusion energy, which has some incredibly positive results.

244
00:18:39,779 --> 00:18:43,902
[SPEAKER_00]: We read an interview yesterday, back from 2016.

245
00:18:58,893 --> 00:19:02,835
[SPEAKER_01]: And that's seriously frightening, but in a way also funny.

246
00:19:02,835 --> 00:19:16,904
[SPEAKER_01]: Altman says, I try not to think too much about it, but I have guns, gold, potassium, iodide, antibiotics, batteries, water, gas mask from Israeli Defense Force and a big patch of land in Big Sur I can fly to.

247
00:19:18,705 --> 00:19:20,046
[SPEAKER_01]: It really spooks me.

248
00:19:20,046 --> 00:19:25,749
[SPEAKER_01]: Yeah, I mean, the CEO of OpenAI says this.

249
00:19:25,749 --> 00:19:26,730
[SPEAKER_02]: I got it.

250
00:19:26,730 --> 00:19:41,498
[SPEAKER_02]: I mean, I think the instinct to prep for the future is maybe best thought of as independent from the technology that they're creating, because it's actually just a kind of engineering approach to the world.

251
00:19:41,498 --> 00:19:43,059
[SPEAKER_02]: When you build complicated systems,

252
00:19:43,859 --> 00:19:48,360
[SPEAKER_02]: You can get 80% of the way there with a straightforward design.

253
00:19:48,360 --> 00:19:51,561
[SPEAKER_02]: But when things break, it's on the edges.

254
00:19:51,561 --> 00:19:53,382
[SPEAKER_02]: It's outlier situations.

255
00:19:53,382 --> 00:20:03,424
[SPEAKER_02]: And good engineers, people who, whether it's they're thinking about the future or millions of users, they're planning for these kind of outside possibilities happening.

256
00:20:03,424 --> 00:20:05,005
[SPEAKER_02]: But is he serious saying this?

257
00:20:05,005 --> 00:20:06,505
[SPEAKER_01]: Or is it kind of a joke?

258
00:20:06,505 --> 00:20:07,706
[SPEAKER_02]: No, I believe he has that.

259
00:20:07,706 --> 00:20:09,106
[SPEAKER_02]: I think he's being totally sincere.

260
00:20:09,106 --> 00:20:11,967
[SPEAKER_02]: So the question is, is he linking

261
00:20:13,067 --> 00:20:18,228
[SPEAKER_02]: the technology that he's working on to the idea that he needs to prep for the apocalypse.

262
00:20:18,228 --> 00:20:22,529
[SPEAKER_02]: No, I think it's just a general kind of paranoia around what's going to happen in the future.

263
00:20:34,412 --> 00:20:40,941
[SPEAKER_00]: And it's very typical for these nerds that they like to prepare for all scenarios and solve problems themselves.

264
00:20:40,941 --> 00:20:43,925
[SPEAKER_00]: And that's not necessarily tied to artificial intelligence.

265
00:20:44,873 --> 00:20:48,056
[SPEAKER_01]: And let's just aim for a hopeful finish here.

266
00:20:48,056 --> 00:20:54,342
[SPEAKER_01]: Make me a believer and convince me that I don't have to brace for Armageddon.

267
00:20:54,342 --> 00:20:58,787
[SPEAKER_01]: But we're heading into a bright future with AI.

268
00:20:58,787 --> 00:21:00,749
[SPEAKER_02]: I can't help you with the doom, man.

269
00:21:00,749 --> 00:21:04,492
[SPEAKER_01]: Not the doom, but the realistic scenario, the optimistic scenario.

270
00:21:04,492 --> 00:21:04,993
[SPEAKER_01]: What would that be?

271
00:21:06,206 --> 00:21:18,774
[SPEAKER_02]: Well, I think it's that we continue to have humans in the loop kind of harnessing the best parts of AI and the AI is ultimately subservient to us.

272
00:21:18,774 --> 00:21:20,155
[SPEAKER_02]: And I think that's possible.

273
00:21:20,155 --> 00:21:23,237
[SPEAKER_02]: I mean, everyone who's building those systems is thinking in those terms.

274
00:21:23,237 --> 00:21:35,545
[SPEAKER_02]: I don't think there's a kind of villainous, I mean, who knows what China's doing, but like, I don't think there's a villainous CEO out there who just wants to delegate everything to the AI and make sure it has, you know, its own motivations and that kind of thing.

275
00:21:36,025 --> 00:21:47,857
[SPEAKER_02]: So the one silver lining of the companies being concentrated or the power being concentrated in these companies is that there's only really a handful of them to regulate and they have the power.

276
00:21:47,857 --> 00:21:58,488
[SPEAKER_02]: And so if OpenAI or Microsoft or Facebook, if we put pressure on them to do the right thing, set up the best practices, hopefully the fact that it's concentrated means that we can control it more.

277
00:21:58,948 --> 00:22:05,853
[SPEAKER_00]: Fred's optimism about the future of AI can best be exemplified by how he works with it himself.

278
00:22:05,853 --> 00:22:16,641
[SPEAKER_00]: He has a small company where he spreads customer service, he develops the product, he does everything himself, and there he uses chatGBT to delegate work tasks so he can do this and make a deal on a crazy idea.

279
00:22:18,002 --> 00:22:18,682
[SPEAKER_00]: Perfect.

280
00:22:18,682 --> 00:22:19,343
[SPEAKER_00]: That was really fun.

281
00:22:45,417 --> 00:22:48,698
[SPEAKER_01]: Fred Benenson, thank you so much for taking time talking to us.

282
00:22:48,698 --> 00:22:51,220
[SPEAKER_01]: I've heard you were going for, what, Louisiana?

283
00:22:51,220 --> 00:22:52,720
[SPEAKER_01]: Well, not the state, but the museum.

284
00:22:52,720 --> 00:22:54,341
[SPEAKER_01]: Yeah.

285
00:22:54,341 --> 00:22:56,382
[SPEAKER_01]: He's going to take the Tesla now and disappear.

286
00:22:56,382 --> 00:22:57,562
[SPEAKER_01]: Oh, he's going to take your Tesla.

287
00:22:57,562 --> 00:22:58,763
[SPEAKER_01]: Yeah, it's a good deal.

288
00:23:07,497 --> 00:23:16,361
[SPEAKER_01]: What does your wife say about you having a living husband?

289
00:23:16,361 --> 00:23:18,322
[SPEAKER_00]: That's off-topic, Marcel.

290
00:23:19,772 --> 00:23:33,348
[SPEAKER_01]: Henrik, we have come to Apple Podcast and Spotify, and we must of course say that we are also in DL Lyd, but more interestingly, we have also received our first application in Apple Podcast.

291
00:23:33,348 --> 00:23:36,031
[SPEAKER_01]: Yes, there is one that has given us one star.

292
00:23:38,113 --> 00:23:39,934
[SPEAKER_01]: It's just to understand.

293
00:23:39,934 --> 00:23:42,955
[SPEAKER_00]: I've heard it and I think it was... I'm sure it was him who thought you spoke too much English.

294
00:23:42,955 --> 00:23:43,515
[SPEAKER_00]: Yes, I think so too.

295
00:23:43,515 --> 00:23:44,895
[SPEAKER_01]: But I think it's fair enough.

296
00:23:44,895 --> 00:23:47,996
[SPEAKER_01]: He thinks it's a bad show because of that.

297
00:23:47,996 --> 00:23:50,837
[SPEAKER_01]: But I want to read a little bit from our first review.

298
00:23:50,837 --> 00:23:52,978
[SPEAKER_01]: It's written by a person called...

299
00:23:53,818 --> 00:23:54,779
[SPEAKER_01]: Molgatti.

300
00:23:54,779 --> 00:24:00,463
[SPEAKER_01]: It sounds a bit like a CIA attack from the 70s or a legendary mortadella chef.

301
00:24:00,463 --> 00:24:10,630
[SPEAKER_01]: We get five stars and the host writes, I apologize, but I do not have specific information about the DR podcast promptly, as my knowledge goes until September 2021.

302
00:24:10,630 --> 00:24:18,035
[SPEAKER_00]: I think maybe you should read it properly, Marcel, because it says, as an A-language model, I cannot evaluate this podcast.

303
00:24:18,435 --> 00:24:25,201
[SPEAKER_00]: And then it says in the text, I'm sorry, but I don't have specific information about this podcast, as my knowledge only goes as far as September 2021.

304
00:24:25,201 --> 00:24:31,767
[SPEAKER_00]: It is possible that it is a newer podcast or one that has not gained widespread attention within this time frame.

305
00:24:31,767 --> 00:24:36,772
[SPEAKER_00]: I recommend checking their official website or other reliable sources for more detailed and updated information about this podcast.

306
00:24:37,032 --> 00:24:42,597
[SPEAKER_01]: Why did you write a review of our show with these ads?

307
00:24:42,597 --> 00:25:00,170
[SPEAKER_00]: It was actually a practical joke a few hours later, because I saw that we had come up, and then I wanted to see if Apple would accept if you used the chat GPT to review it, so I wrote a prompt to chat GPT, evaluate, review, evaluate this podcast, and then I got this answer, and then I actually wrote the headline myself, because

308
00:25:01,352 --> 00:25:16,730
[SPEAKER_00]: It's a meme, a thing that people have fun about on the internet for a long time, that people who have used ChatGPT sometimes forget to remove a sentence that in English reads, as an AI language model, I cannot, and then it explains why it can't answer a question.

309
00:25:16,730 --> 00:25:20,474
[SPEAKER_00]: And there are a lot of people who have found examples of this in everything from

310
00:25:21,335 --> 00:25:28,119
[SPEAKER_00]: I mean, some user references, for example, like podcasts, but also on Amazon and on Yelp in the USA.

311
00:25:28,119 --> 00:25:30,501
[SPEAKER_00]: I have also found it on some Danish websites.

312
00:25:30,501 --> 00:25:39,927
[SPEAKER_00]: And it has become such a thing that you can find on Twitter, where some bots write false tweets, often where they look a lot like dressed women.

313
00:25:39,927 --> 00:25:41,888
[SPEAKER_00]: It has become such a whole thing to find them.

314
00:25:41,888 --> 00:25:49,113
[SPEAKER_00]: I mean, specialized scientific articles have this sentence in them, and it is a way of how you can see

315
00:25:49,673 --> 00:25:52,694
[SPEAKER_00]: that chat GPT has been used and therefore does not want to answer the questions.

316
00:25:52,694 --> 00:26:00,478
[SPEAKER_01]: But it's super interesting that we are on our way to an internet that is read with all this AI generated crap.

317
00:26:00,478 --> 00:26:02,979
[SPEAKER_01]: Reviews are a pretty important resource in a way.

318
00:26:02,979 --> 00:26:05,560
[SPEAKER_01]: It is in the way that we as consumers orient ourselves around.

319
00:26:05,560 --> 00:26:10,422
[SPEAKER_01]: What is there to say about this product, apart from what they themselves write to the producer, right?

320
00:26:10,422 --> 00:26:13,023
[SPEAKER_01]: And the credibility that is in the reviews, it is about to be destroyed in a way.

321
00:26:13,023 --> 00:26:14,204
[SPEAKER_01]: I mean, it's probably broken.

322
00:26:14,204 --> 00:26:15,785
[SPEAKER_01]: It has been destroyed for a long time, Marcel.

323
00:26:15,785 --> 00:26:16,345
[SPEAKER_01]: I'm sorry to say it.

324
00:26:16,645 --> 00:26:19,266
[SPEAKER_01]: But what's the purpose of all these posts?

325
00:26:19,266 --> 00:26:22,326
[SPEAKER_01]: Except for yours, of course, which was the Troll-app.

326
00:26:22,326 --> 00:26:24,727
[SPEAKER_01]: But what's the purpose of all these fake posts?

327
00:26:24,727 --> 00:26:27,887
[SPEAKER_00]: Well, I found, for example, a website that was kind of a link-farm.

328
00:26:27,887 --> 00:26:33,689
[SPEAKER_00]: Those were the old days, where you put a lot of text on a page that was about something to get traffic in, and then you served some ads.

329
00:26:33,689 --> 00:26:34,229
[SPEAKER_00]: Can I see it?

330
00:26:34,229 --> 00:26:35,989
[SPEAKER_00]: Yes, let me find it here.

331
00:26:35,989 --> 00:26:36,289
[SPEAKER_00]: Yes.

332
00:26:36,289 --> 00:26:38,970
[SPEAKER_00]: It's called tv- og internet.dk.

333
00:26:38,970 --> 00:26:42,911
[SPEAKER_00]: All you need to know about Danish homepages, TV and Internet packages.

334
00:26:42,911 --> 00:26:44,551
[SPEAKER_00]: Can I see it?

335
00:26:44,551 --> 00:26:45,071
[SPEAKER_00]: It comes here.

336
00:26:45,883 --> 00:26:51,024
[SPEAKER_00]: And I found that by googling the sentence as an AI-language model.

337
00:26:51,024 --> 00:26:51,944
[SPEAKER_01]: Yes.

338
00:26:51,944 --> 00:26:56,865
[SPEAKER_01]: There's a picture of a remote control, and then there's pretty much everything you need to know about TV and the Internet.

339
00:26:56,865 --> 00:27:07,967
[SPEAKER_00]: And it has a whole bunch of sub-pages about TV and the Internet in Fjerretslev, TV and the Internet in Brabrand, TV... I mean, for every location in Denmark, they've just automated and made a whole bunch of sub-pages.

340
00:27:07,967 --> 00:27:13,508
[SPEAKER_00]: So when someone wants to find an Internet connection in their local area, they can go to this page here.

341
00:27:14,108 --> 00:27:19,449
[SPEAKER_00]: And then there's a whole bunch of text that apparently seems reasonable, but is completely ridiculous.

342
00:27:19,449 --> 00:27:23,070
[SPEAKER_00]: The homepage now has a sub-section called, How do Danish homepages work?

343
00:27:23,070 --> 00:27:30,912
[SPEAKER_00]: And then it starts, As an AI language model, I can tell that Danish homepages work in Danish by using Danish as the primary language on the website.

344
00:27:30,912 --> 00:27:35,913
[SPEAKER_00]: This includes everything from navigation menus, transcripts, spreadsheets and buttons to contact forms and payment processes.

345
00:27:35,913 --> 00:27:43,075
[SPEAKER_00]: In addition, Danish homepages often adapt the Danish culture and ways of trading, for example, with the typical Danish currency as standard currency, and then it just continues.

346
00:27:43,975 --> 00:27:46,218
[SPEAKER_00]: It's a robot home page.

347
00:27:46,218 --> 00:27:49,020
[SPEAKER_00]: And then I looked at an even more local home page.

348
00:27:49,020 --> 00:28:00,772
[SPEAKER_00]: It said, for example, technology that is able to send Wi-Fi signals directly to devices, that this is an alternative to signals in your society of a placeholder, that means that the unit you run water on Netflix.

349
00:28:01,593 --> 00:28:02,334
[SPEAKER_00]: It really became black.

350
00:28:02,334 --> 00:28:03,376
[SPEAKER_00]: It's pretty black, yes.

351
00:28:03,376 --> 00:28:04,137
[SPEAKER_00]: I understand that.

352
00:28:04,137 --> 00:28:10,346
[SPEAKER_00]: And I think it's just a combination of having machine-translated, ungenerated text, and not checking it afterwards at all.

353
00:28:10,346 --> 00:28:14,372
[SPEAKER_01]: It's fascinating that we're on our way to a robot world, where robots are reading the robot's home.

354
00:28:16,595 --> 00:28:18,376
[SPEAKER_01]: And the robot is listening to the robot's music.

355
00:28:18,376 --> 00:28:29,745
[SPEAKER_00]: This is a robot that is trying to create a website that will appear on top of the Google search results, so that you have to go in and click on a link to set up an internet connection at Nordlys, or Stofa, or QuickNet.

356
00:28:29,745 --> 00:28:33,388
[SPEAKER_01]: And then there's a man in a teenager's MV making money on me clicking on that link.

357
00:28:33,388 --> 00:28:38,992
[SPEAKER_00]: And then they get a kickback, a small receipt for every time someone signs up for an internet connection.

358
00:28:40,248 --> 00:28:46,952
[SPEAKER_01]: How can it be that it is so difficult to discover if something is written by an AI or a human?

359
00:28:46,952 --> 00:28:54,596
[SPEAKER_01]: It is of course the dilemma that school teachers and university researchers sit with when they get these assignments written by ChatGPT at the door.

360
00:28:54,596 --> 00:29:02,541
[SPEAKER_01]: It may be easy to get an idea that it is written by ChatGPT, but it is partly difficult to prove that it actually is.

361
00:29:02,541 --> 00:29:04,382
[SPEAKER_01]: It's not plagiarism control in that way.

362
00:29:04,382 --> 00:29:06,463
[SPEAKER_01]: There was also a story before.

363
00:29:07,283 --> 00:29:16,906
[SPEAKER_01]: The Verge among others, and a lot of other media, about how OpenAI was developing a tool to detect what is made by a human, what is made by a robot.

364
00:29:16,906 --> 00:29:21,008
[SPEAKER_01]: But they have, without making a big deal out of it, in silence, shut down this tool.

365
00:29:21,008 --> 00:29:24,349
[SPEAKER_01]: And the reason was simply too low precision.

366
00:29:24,349 --> 00:29:25,729
[SPEAKER_01]: The tool was simply too bad.

367
00:29:25,729 --> 00:29:27,910
[SPEAKER_01]: It could not distinguish between text made by humans and AI.

368
00:29:28,750 --> 00:29:32,611
[SPEAKER_01]: Why is it technically so difficult to make this tool?

369
00:29:32,611 --> 00:29:40,114
[SPEAKER_00]: Well, what a chatbot does is that it basically searches for what it has read in a whole bunch of texts that it has been served as training data.

370
00:29:40,114 --> 00:29:50,217
[SPEAKER_00]: That means that it will statistically go out from some calculations it has made every time, what is the most likely answer to this question, and therefore it will often generate unique content.

371
00:29:50,217 --> 00:29:57,040
[SPEAKER_00]: The less you ask about something very specific that it has very little data to answer from, the more you will get different answers every time.

372
00:29:57,660 --> 00:30:09,705
[SPEAKER_00]: And the reason why you can use this sentence as an ES-model, is that it's only when it doesn't want to answer for some reason, for example ethical or other reasons, that it gives this recognizable sentence.

373
00:30:09,705 --> 00:30:11,445
[SPEAKER_00]: And it doesn't do that all the time.

374
00:30:11,445 --> 00:30:16,247
[SPEAKER_00]: So therefore, it will be a different text you get every time, and it dictates, it finds everything.

375
00:30:16,247 --> 00:30:21,129
[SPEAKER_00]: There's a researcher who has called generative AI bullshit machines.

376
00:30:21,129 --> 00:30:23,430
[SPEAKER_00]: They can talk and let the background go.

377
00:30:24,410 --> 00:30:34,937
[SPEAKER_00]: But they don't really know what they're saying, and that's why it's very difficult to detect, because they're so good at making it sound very, very real, but it never is.

378
00:30:34,937 --> 00:30:35,557
[SPEAKER_00]: Does that make sense?

379
00:30:35,557 --> 00:30:36,418
[SPEAKER_01]: Yes, it makes sense.

380
00:30:36,418 --> 00:30:48,285
[SPEAKER_01]: It reminds me a bit of if you were to have a job interview with someone who just comes in and likes a whole bunch of bullshit, then you would expose the applicant for some tests, maybe, or something like that, which could reveal that this is bullshit.

381
00:30:51,427 --> 00:30:54,751
[SPEAKER_00]: And that's exactly what OpenAI has done with this Classifier tool.

382
00:30:54,751 --> 00:30:58,675
[SPEAKER_00]: They have taken some texts that they knew were written by ChatGPT.

383
00:30:58,675 --> 00:31:00,737
[SPEAKER_00]: They have taken some texts that they didn't know were.

384
00:31:00,737 --> 00:31:02,940
[SPEAKER_00]: And I think it was 29% of the time they could detect

385
00:31:08,285 --> 00:31:12,686
[SPEAKER_00]: when it was generated, it was simply too bad for bad results.

386
00:31:12,686 --> 00:31:14,987
[SPEAKER_00]: And therefore they have chosen to postpone the project.

387
00:31:14,987 --> 00:31:16,408
[SPEAKER_00]: And I think that's actually very sensible.

388
00:31:16,408 --> 00:31:31,852
[SPEAKER_01]: Yes, of course, there is nothing wrong with that decision, but it is more that it is perhaps quite worrying that the company that is far ahead, perhaps, with generative AI right now, cannot develop a tool that can reveal whether what this robot has done is generated by AI or not.

389
00:31:31,852 --> 00:31:33,173
[SPEAKER_01]: So it's pretty scary in a way.

390
00:31:33,173 --> 00:31:35,113
[SPEAKER_01]: Yes, and interesting too.

391
00:31:35,113 --> 00:31:36,694
[SPEAKER_01]: How much of your racing cycles actually

392
00:31:39,588 --> 00:31:44,329
[SPEAKER_00]: I haven't bought new ones recently, so I can't deliver any very, very high rates.

393
00:31:44,329 --> 00:31:44,950
[SPEAKER_00]: Okay.

394
00:31:44,950 --> 00:31:46,310
[SPEAKER_00]: What about Tesla?

395
00:31:46,310 --> 00:31:47,650
[SPEAKER_00]: I've leased it myself.

396
00:31:47,650 --> 00:31:49,951
[SPEAKER_01]: You won't get me this time.

397
00:31:49,951 --> 00:31:57,633
[SPEAKER_01]: Well, I can inform you that FTC, the Federal Trade Commission of the United States, will shut down what they call dishonest reporting practices.

398
00:31:58,653 --> 00:32:01,654
[SPEAKER_01]: For example, false reports can also be read about on The Verge.

399
00:32:01,654 --> 00:32:09,077
[SPEAKER_01]: FTC mentions the specific origin of AI chatbots as something that makes it easier to make false reports.

400
00:32:09,077 --> 00:32:11,458
[SPEAKER_01]: And the payment size can go up to $50,000.

401
00:32:11,458 --> 00:32:13,759
[SPEAKER_01]: That's a little more than my bike.

402
00:32:13,759 --> 00:32:15,520
[SPEAKER_01]: Yes, but not more than your monthly salary.

403
00:32:15,520 --> 00:32:15,860
[SPEAKER_00]: Yes, it is.

404
00:32:16,200 --> 00:32:26,003
[SPEAKER_00]: But I think it's going to have a devastating effect, because it's actually the case that applications, also here in Denmark, but to an even greater extent in the USA, have been completely impossible to trust for a very, very long time.

405
00:32:26,003 --> 00:32:28,884
[SPEAKER_00]: I mean, what you call organic applications, i.e.

406
00:32:28,884 --> 00:32:34,286
[SPEAKER_00]: applications that have been written by real people who are honest, have been largely useless for a long time.

407
00:32:34,286 --> 00:32:36,507
[SPEAKER_01]: I'm looking forward to seeing if you get a ticket for this.

408
00:32:36,507 --> 00:32:38,667
[SPEAKER_01]: I'll be happy if it happens.

409
00:32:38,667 --> 00:32:39,988
[SPEAKER_01]: We like real applications.

410
00:32:40,990 --> 00:32:49,054
[SPEAKER_01]: So if you like the show, go to Spotify or Apple Podcasts and give us a like for every one you get.

411
00:32:49,054 --> 00:32:51,736
[SPEAKER_01]: That was episode 2 of Prompt.

412
00:32:51,736 --> 00:32:52,976
[SPEAKER_01]: I think you did a great job, Henrik.

413
00:32:52,976 --> 00:32:54,277
[SPEAKER_01]: You're very much like me.

414
00:32:54,277 --> 00:32:54,937
[SPEAKER_01]: At least I got smarter.

415
00:32:54,937 --> 00:32:57,739
[SPEAKER_01]: We're coming out every Thursday on the podcast IdeaLyd.

416
00:32:57,739 --> 00:32:59,299
[SPEAKER_01]: You can listen to it from the morning show.

417
00:32:59,299 --> 00:33:07,844
[SPEAKER_01]: I was wondering if we could meet this weekend and listen to the rest of your harddisk collection.

418
00:33:07,844 --> 00:33:09,525
[SPEAKER_00]: I don't think we need that, Marcel.

